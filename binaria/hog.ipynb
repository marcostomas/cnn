{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rede neural CNN para a tarefa de classificação binária utilizando o extrator de dados HOG (Histogram of Oriented Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar as bibliotecas e os dados do conjunto MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from skimage.feature import hog\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Carregar o conjunto de dados MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando a função que implementa o descritor HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(images):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        fd = hog(image, orientations=8, pixels_per_cell=(4, 4),cells_per_block=(1, 1), visualize=False)\n",
    "        features.append(fd)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar os dados. As imagens já vem em escala de cinza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraír as características HOG do conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_hog = extract_hog_features(x_train)\n",
    "x_test_hog = extract_hog_features(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando as características HOG extraídas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_hog = scaler.fit_transform(x_train_hog)\n",
    "x_test_hog = scaler.transform(x_test_hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar os filtros de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas duas classes (por exemplo, 0 e 1)\n",
    "binary_classes = [0, 1]\n",
    "train_filter = np.where((y_train == binary_classes[0]) | (y_train == binary_classes[1]))\n",
    "test_filter = np.where((y_test == binary_classes[0]) | (y_test == binary_classes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_binary, y_train_binary = x_train_hog[train_filter], y_train[train_filter]\n",
    "x_test_binary, y_test_binary = x_test_hog[test_filter], y_test[test_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converter os rótulos para one-hot encoding\n",
    "\n",
    "`to_categorical(rótulos, número de classes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = to_categorical(y_train_binary, 2)\n",
    "y_test_binary = to_categorical(y_test_binary, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(x_train_hog.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criando e salvando os hiperparâmetros da arquitetura e da inicialização em formato json em arquivos-hog/hiperparametros.json\n",
    "\n",
    "- Salvando os pesos iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os hiperparâmetros\n",
    "hiperparametros = {\n",
    "    \"architecture\": {\n",
    "        \"input_shape\": (28, 28, 1),\n",
    "        \"conv_layers\": [\n",
    "            {\"filters\": 32, \"kernel_size\": (3, 3), \"activation\": \"relu\"},\n",
    "            {\"filters\": 64, \"kernel_size\": (3, 3), \"activation\": \"relu\"}\n",
    "        ],\n",
    "        \"pooling_layers\": [{\"pool_size\": (2, 2)}] * 2,\n",
    "        \"dropout_rates\": [0.25, 0.25, 0.5],\n",
    "        \"dense_layers\": [{\"units\": 128, \"activation\": \"relu\"}],\n",
    "        \"output_layer\": {\"units\": 2, \"activation\": \"softmax\"}\n",
    "    },\n",
    "    \"initialization\": {\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 10,\n",
    "        \"validation_split\": 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Serializando os hiperparâmetros em uma string JSON\n",
    "hiperparametros_json = json.dumps(hiperparametros, indent=4)\n",
    "\n",
    "# Escrevendo a string JSON em um arquivo\n",
    "with open(\"arquivos-hog/hiperparametros.json\", \"w\") as arquivo:\n",
    "    arquivo.write(hiperparametros_json)\n",
    "\n",
    "\n",
    "# PESOS INICIAIS\n",
    "model.save_weights('arquivos-hog/pesos_iniciais.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9587 - loss: 0.1168 - val_accuracy: 0.9996 - val_loss: 0.0013\n",
      "Epoch 2/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 0.9996 - val_loss: 0.0015\n",
      "Epoch 3/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0042 - val_accuracy: 0.9980 - val_loss: 0.0048\n",
      "Epoch 4/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0029 - val_accuracy: 0.9996 - val_loss: 9.0932e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 9.8413e-04 - val_accuracy: 1.0000 - val_loss: 1.4163e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0012 - val_accuracy: 0.9996 - val_loss: 8.9889e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 7.1031e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.9996 - val_loss: 5.8875e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 7.5206e-04 - val_accuracy: 0.9996 - val_loss: 0.0011\n",
      "Epoch 10/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0012 - val_accuracy: 0.9988 - val_loss: 0.0052\n"
     ]
    }
   ],
   "source": [
    "historico = model.fit(x_train_binary, y_train_binary, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvando os pesos finais da rede \n",
    "- Salvando o histórico de perda para cada iteração\n",
    "- Salvando as saídas produzidas pela rede para cada um dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# PESOS FINAIS\n",
    "model.save_weights('arquivos-hog/pesos_finais.weights.h5')\n",
    "\n",
    "# ERRO DE CADA ITERAÇÃO\n",
    "perdas = historico.history['loss']\n",
    "\n",
    "# Salvando o histórico de perda em um arquivo JSON\n",
    "with open('arquivos-hog/historico_perda.json', 'w') as f:\n",
    "    json.dump(perdas, f)\n",
    "\n",
    "\n",
    "# SAÍDAS PRODUZIDAS\n",
    "# Fazendo inferência com o modelo treinado para obter as saídas\n",
    "saidas = model.predict(x_train_hog)\n",
    "\n",
    "# Convertendo as saídas para uma lista para serialização\n",
    "saidas_lista = saidas.tolist()\n",
    "\n",
    "# Salvando as saídas em um arquivo JSON\n",
    "with open('arquivos-hog/saidas_teste.json', 'w') as f:\n",
    "    json.dump(saidas_lista, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.2354e-05\n",
      "Test accuracy: 0.9995272159576416, Test loss: 0.0007050381973385811\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test_binary, y_test_binary)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTAS\n",
    "\n",
    "O descritor HOG é uma técnica de extração de características usada para pré-processar imagens antes de alimentá-las a um modelo de aprendizado de máquina, que pode ser uma Rede Neural Densa, uma Máquina de Vetores de Suporte (SVM), entre outros, mas não é específico para simplificar ou modificar a arquitetura de uma CNN.\n",
    "\n",
    "As CNNs são projetadas para aprender automaticamente hierarquias de características a partir dos dados brutos de entrada durante o treinamento. As camadas convolucionais e de pooling em uma CNN são capazes de extrair características importantes das imagens automaticamente, o que torna o uso de técnicas manuais de extração de características, como o HOG, geralmente desnecessário para este tipo de rede.\n",
    "\n",
    "Portanto, a menção ao descritor HOG estava relacionada a uma abordagem alternativa de processamento de imagens para modelos que não são capazes de extração automática de características, e não como uma técnica para remover camadas de uma CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
