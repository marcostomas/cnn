{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rede neural CNN treinado sob dados brutos de forma binária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar as bibliotecas e os dados do conjunto MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1. Carregar o conjunto de dados MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "#print (x_train[0][1]) # [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "#print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar e redimensionar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar os filtros de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas duas classes (por exemplo, 0 e 1)\n",
    "binary_classes = [0, 1]\n",
    "train_filter = np.where((y_train == binary_classes[0]) | (y_train == binary_classes[1]))\n",
    "test_filter = np.where((y_test == binary_classes[0]) | (y_test == binary_classes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train[train_filter], y_train[train_filter]\n",
    "x_test, y_test = x_test[test_filter], y_test[test_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construir o modelo da CNN**\n",
    "\n",
    "Este modelo é uma rede neural convolucional (CNN) construída usando a API Sequential do Keras. Ele é projetado para processar imagens de entrada com a forma (28, 28, 1), o que sugere imagens em escala de cinza de 28x28 pixels. Aqui está uma explicação passo a passo de cada camada:\n",
    "\n",
    "Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)): Esta é a primeira camada convolucional. Ela tem 32 filtros (ou kernels), cada um com um tamanho de (3, 3). A função de ativação ReLU é usada. input_shape=(28, 28, 1) define a forma das imagens de entrada que a rede espera, que são imagens de 28x28 pixels em escala de cinza (1 canal de cor).\n",
    "\n",
    "MaxPooling2D((2, 2)): Uma camada de pooling que reduz as dimensões espaciais (altura e largura) da saída da camada anterior pela metade. Isso é feito usando uma janela de pooling de tamanho (2, 2) e selecionando o valor máximo dentro dessa janela.\n",
    "\n",
    "Conv2D(64, (3, 3), activation='relu'): Outra camada convolucional com 64 filtros de tamanho (3, 3) e função de ativação ReLU. Não é necessário especificar o formato de entrada aqui, pois o Keras infere automaticamente a partir da saída da camada anterior.\n",
    "\n",
    "MaxPooling2D((2, 2)): Mais uma camada de pooling que reduz as dimensões espaciais da saída da camada anterior pela metade, usando a mesma janela de pooling de tamanho (2, 2).\n",
    "\n",
    "Conv2D(64, (3, 3), activation='relu'): Uma terceira camada convolucional com 64 filtros de tamanho (3, 3) e função de ativação ReLU. Isso aumenta ainda mais a capacidade da rede de aprender características complexas.\n",
    "\n",
    "Flatten(): Esta camada achata a saída tridimensional da última camada convolucional em um vetor unidimensional. Isso é necessário porque as próximas camadas, as camadas densas, esperam entradas unidimensionais.\n",
    "\n",
    "Dense(64, activation='relu'): Uma camada densa (ou totalmente conectada) com 64 unidades e função de ativação ReLU. Ela recebe o vetor achatado da camada anterior como entrada.\n",
    "\n",
    "Dense(1, activation='sigmoid'): A última camada é uma camada densa com uma única unidade e a função de ativação sigmoid. Isso é típico para problemas de classificação binária, onde a saída é a probabilidade de a entrada pertencer a uma das duas classes.\n",
    "\n",
    "Este modelo é adequado para tarefas de classificação binária de imagens, como determinar se uma imagem contém um certo objeto ou não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar o modelo - https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\n",
    "\n",
    "Utiliza o otimizador \"adam\" - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "\n",
    "Este código configura o modelo para o treinamento, especificando o otimizador, a função de perda e as métricas a serem avaliadas durante o treinamento e a validação:\n",
    "\n",
    "optimizer='adam': Define o otimizador Adam para ajustar os pesos da rede. Adam é um algoritmo de otimização baseado em gradiente que é eficiente em termos de computação e tem um requisito de memória relativamente baixo. É amplamente usado por ser eficaz em uma ampla gama de problemas de aprendizado de máquina.\n",
    "\n",
    "loss='binary_crossentropy': Especifica a função de perda como entropia cruzada binária. Esta é uma escolha comum para problemas de classificação binária. A entropia cruzada binária mede o desempenho do modelo cuja saída é um valor de probabilidade entre 0 e 1. Ela compara a distribuição de probabilidade prevista pela rede com a distribuição real (os rótulos verdadeiros).\n",
    "\n",
    "metrics=['accuracy']: Define a métrica de avaliação do modelo como precisão (accuracy). A precisão é a fração de previsões corretas entre o total de previsões feitas. É uma métrica comum para avaliar o desempenho de modelos em tarefas de classificação.\n",
    "\n",
    "Ao chamar model.compile(), você está preparando o modelo para o treinamento, compilando-o com as configurações especificadas. Isso inclui preparar as estruturas de dados internas do modelo e se preparar para a otimização (ajuste dos pesos da rede durante o treinamento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criando e salvando os hiperparâmetros da arquitetura e da inicialização em formato json em arquivos-brutos/hiperparametros.json\n",
    "\n",
    "- Salvando os pesos iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os hiperparâmetros da arquitetura da rede neural e os hiperparâmetros de inicialização\n",
    "hiperparametros = {\n",
    "    'arquitetura': {\n",
    "        'camadas': [\n",
    "            {'tipo': 'Conv2D', 'filtros': 32, 'kernel_size': (3, 3), 'activation': 'relu', 'input_shape': (28, 28, 1)},\n",
    "            {'tipo': 'MaxPooling2D', 'pool_size': (2, 2)},\n",
    "            {'tipo': 'Conv2D', 'filtros': 64, 'kernel_size': (3, 3), 'activation': 'relu'},\n",
    "            {'tipo': 'MaxPooling2D', 'pool_size': (2, 2)},\n",
    "            {'tipo': 'Conv2D', 'filtros': 64, 'kernel_size': (3, 3), 'activation': 'relu'},\n",
    "            {'tipo': 'Flatten'},\n",
    "            {'tipo': 'Dense', 'unidades': 64, 'activation': 'relu'},\n",
    "            {'tipo': 'Dense', 'unidades': 1, 'activation': 'sigmoid'}\n",
    "        ]\n",
    "    },\n",
    "    'inicializacao': {\n",
    "        'optimizer': 'adam',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['accuracy']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Serializando os hiperparâmetros em uma string JSON\n",
    "hiperparametros_json = json.dumps(hiperparametros, indent=4)\n",
    "\n",
    "# Escrevendo a string JSON em um arquivo\n",
    "with open(\"arquivos-brutos/hiperparametros.json\", \"w\") as arquivo:\n",
    "    arquivo.write(hiperparametros_json)\n",
    "\n",
    "\n",
    "# PESOS INICIAIS\n",
    "model.save_weights('arquivos-brutos/pesos_iniciais.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinar o modelo**\n",
    "\n",
    "Este trecho de código inicia o treinamento do modelo com os dados fornecidos, especificando também como o treinamento deve ser realizado. Aqui está o que cada argumento significa:\n",
    "\n",
    "x_train: Os dados de entrada para treinar o modelo. Estes são os exemplos que o modelo usará para aprender.\n",
    "\n",
    "y_train: As etiquetas (rótulos) correspondentes aos dados de entrada x_train. Estas são as respostas corretas que o modelo tentará prever.\n",
    "\n",
    "epochs=15: Define o número de épocas de treinamento. Uma época significa uma iteração sobre todos os dados de entrada. Portanto, o modelo passará pelos dados de treinamento 15 vezes.\n",
    "\n",
    "batch_size=64: Especifica o tamanho do lote (batch size). Isso significa que 64 exemplos de x_train e y_train serão usados para atualizar os pesos do modelo de uma vez. O uso de lotes ajuda a acelerar o treinamento e pode contribuir para uma melhor generalização do modelo.\n",
    "\n",
    "validation_split=0.2: Este argumento reserva 20% dos dados de treinamento para validação. O modelo não treinará nesses dados. Em vez disso, após cada época, ele avaliará seu desempenho nesse subconjunto de validação. Isso é útil para monitorar o overfitting (quando o modelo aprende padrões específicos dos dados de treinamento, mas falha em generalizar para novos dados).\n",
    "\n",
    "Portanto, este código treina o modelo nos dados fornecidos (x_train e y_train), por 15 épocas, usando lotes de 64 exemplos, e avalia o desempenho do modelo em 20% dos dados de treinamento reservados para validação após cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9901 - loss: 0.0418 - val_accuracy: 0.9984 - val_loss: 0.0023\n",
      "Epoch 2/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0040 - val_accuracy: 0.9996 - val_loss: 7.5968e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 0.9996 - val_loss: 5.0483e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9995 - loss: 9.4637e-04 - val_accuracy: 1.0000 - val_loss: 7.8426e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 2.8816e-04 - val_accuracy: 0.9988 - val_loss: 0.0063\n",
      "Epoch 6/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9988 - loss: 0.0030 - val_accuracy: 0.9996 - val_loss: 4.0011e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.4025e-04 - val_accuracy: 1.0000 - val_loss: 5.1549e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9993 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 3.3139e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 2.3618e-04 - val_accuracy: 1.0000 - val_loss: 4.9576e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1014/1014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.7619e-06 - val_accuracy: 0.9996 - val_loss: 8.1854e-04\n"
     ]
    }
   ],
   "source": [
    "historico = model.fit(x_train, y_train, epochs=10, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvando os pesos finais da rede \n",
    "- Salvando o histórico de perda para cada iteração\n",
    "- Salvando as saídas produzidas pela rede para cada um dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m396/396\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# PESOS FINAIS\n",
    "model.save_weights('arquivos-brutos/pesos_finais.weights.h5')\n",
    "\n",
    "# ERRO DE CADA ITERAÇÃO\n",
    "perdas = historico.history['loss']\n",
    "\n",
    "# Salvando o histórico de perda em um arquivo JSON\n",
    "with open('arquivos-brutos/historico_perda.json', 'w') as f:\n",
    "    json.dump(perdas, f)\n",
    "\n",
    "\n",
    "# SAÍDAS PRODUZIDAS\n",
    "# Fazendo inferência com o modelo treinado para obter as saídas\n",
    "saidas = model.predict(x_train)\n",
    "\n",
    "# Convertendo as saídas para uma lista para serialização\n",
    "saidas_lista = saidas.tolist()\n",
    "\n",
    "# Salvando as saídas em um arquivo JSON\n",
    "with open('arquivos-brutos/saidas_teste.json', 'w') as f:\n",
    "    json.dump(saidas_lista, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 5.6769e-04\n",
      "Test accuracy: 0.9990543723106384, Test loss: 0.00460610119625926\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
