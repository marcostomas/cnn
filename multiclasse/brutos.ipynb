{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui montaremos a rede para avaliar os dados para a tarefa de classificação multiclasse com a utilização dos dados brutos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar as bibliotecas e os dados do conjunto MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Carregar os dados\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar e redimensionar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converter os rótulos para one-hot encoding\n",
    "\n",
    "`to_categorical(rótulos, número de classes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criando e salvando os hiperparâmetros da arquitetura e da inicialização em formato json em arquivos-brutos/hiperparametros.json\n",
    "\n",
    "- Salvando os pesos iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIPERPARÂMETROS\n",
    "hiperparametros = {\n",
    "    \"arquitetura\": {\n",
    "        \"camadas\": [\n",
    "            {\"tipo\": \"Conv2D\", \"filtros\": 32, \"kernel_size\": 3, \"ativacao\": \"relu\"},\n",
    "            {\"tipo\": \"MaxPooling2D\", \"pool_size\": 2},\n",
    "            {\"tipo\": \"Conv2D\", \"filtros\": 64, \"kernel_size\": 3, \"ativacao\": \"relu\"},\n",
    "            {\"tipo\": \"MaxPooling2D\", \"pool_size\": 2},\n",
    "            {\"tipo\": \"Flatten\"},\n",
    "            {\"tipo\": \"Dense\", \"unidades\": 64, \"ativacao\": \"relu\"},\n",
    "            {\"tipo\": \"Dense\", \"unidades\": 10, \"ativacao\": \"softmax\"}\n",
    "        ]\n",
    "    },\n",
    "    \"inicializacao\": {\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Serializando os hiperparâmetros em uma string JSON\n",
    "hiperparametros_json = json.dumps(hiperparametros, indent=4)\n",
    "\n",
    "# Escrevendo a string JSON em um arquivo\n",
    "with open(\"arquivos-brutos/hiperparametros.json\", \"w\") as arquivo:\n",
    "    arquivo.write(hiperparametros_json)\n",
    "\n",
    "\n",
    "# PESOS INICIAIS\n",
    "model.save_weights('arquivos-brutos/pesos_iniciais.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvando os pesos finais da rede \n",
    "- Salvando o histórico de perda para cada iteração\n",
    "- Salvando as saídas produzidas pela rede para cada um dos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PESOS FINAIS\n",
    "model.save_weights('arquivos-brutos/pesos_finais.weights.h5')\n",
    "\n",
    "# ERRO DE CADA ITERAÇÃO\n",
    "perdas = historico.history['loss']\n",
    "\n",
    "# Salvando o histórico de perda em um arquivo JSON\n",
    "with open('arquivos-brutos/historico_perda.json', 'w') as f:\n",
    "    json.dump(perdas, f)\n",
    "\n",
    "\n",
    "# SAÍDAS PRODUZIDAS\n",
    "# Fazendo inferência com o modelo treinado para obter as saídas\n",
    "saidas = model.predict(x_test)\n",
    "\n",
    "# Convertendo as saídas para uma lista para serialização\n",
    "saidas_lista = saidas.tolist()\n",
    "\n",
    "# Salvando as saídas em um arquivo JSON\n",
    "with open('arquivos-brutos/saidas_teste.json', 'w') as f:\n",
    "    json.dump(saidas_lista, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando a matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "classes_previstas = np.argmax(saidas, axis=1)\n",
    "classes_verdadeiras = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calcule a matriz de confusão\n",
    "cm = confusion_matrix(classes_verdadeiras, classes_previstas)\n",
    "\n",
    "# Plot a matriz de confusão\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTAS:\n",
    "\n",
    "Para criar um modelo de CNN (Rede Neural Convolucional) para o conjunto de dados MNIST, que é um conjunto de dígitos manuscritos de 0 a 9, e portanto um problema de classificação multiclasse, seguiremos os seguintes passos em pseudocódigo:\n",
    "\n",
    "Importar as bibliotecas necessárias.\n",
    "Carregar o conjunto de dados MNIST.\n",
    "Pré-processar os dados:\n",
    "Normalizar os valores dos pixels para o intervalo de 0 a 1.\n",
    "Redimensionar as imagens, se necessário, para garantir que todas tenham o mesmo tamanho.\n",
    "Converter os rótulos (labels) para uma representação \"one-hot\" para a classificação multiclasse.\n",
    "Construir o modelo da CNN:\n",
    "Adicionar camadas convolucionais para extrair características das imagens.\n",
    "Utilizar camadas de pooling para reduzir a dimensionalidade.\n",
    "Adicionar camadas de dropout para reduzir o overfitting.\n",
    "Incluir uma camada achatada (Flatten) para transformar os mapas de características em um vetor.\n",
    "Adicionar uma ou mais camadas densas (fully connected) para a classificação.\n",
    "A última camada densa deve ter um número de neurônios igual ao número de classes (10 para MNIST) e utilizar a função de ativação softmax para a classificação multiclasse.\n",
    "Compilar o modelo:\n",
    "Escolher um otimizador (por exemplo, Adam).\n",
    "Definir a função de perda (loss function) como 'categorical_crossentropy' para classificação multiclasse.\n",
    "Especificar métricas para monitorar, como 'accuracy'.\n",
    "Treinar o modelo com os dados de treinamento.\n",
    "Avaliar o modelo com os dados de teste para verificar a precisão.\n",
    "Agora, vamos transformar esse pseudocódigo em código Python usando TensorFlow e Keras:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
